{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockName= \"GE\"\n",
    "#loading the data from the folder and setting it up\n",
    "df = pd.read_csv(os.path.join('Stocks',stockName+'.us.txt'),delimiter=',',usecols=['Date','Open','High','Low','Close'])\n",
    "num_elem = int((sum(1 for line in open('Stocks/'+stockName+'.us.txt')) -1)/10)*10\n",
    "print\n",
    "print('Loaded data, total number of elements: ', num_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SORTING THE DATA BY DATE FOR LESS HEADACHE\n",
    "df = df.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the order of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the stock just to have an idea of what it looks like\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "plt.title(stockName+\" Stock\",fontsize=20)\n",
    "plt.xlabel('Date',fontsize=20)\n",
    "plt.ylabel('Value',fontsize=20)\n",
    "plt.savefig('pics/'+stockName+'_price_history.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating mid prices\n",
    "mid_prices = (df.loc[:,'High'].values + df.loc[:,'Low'].values) /2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting training set vs test data\n",
    "d_train = mid_prices[:int(num_elem *.9) ]\n",
    "d_test = mid_prices[int(num_elem*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data using minMaxScaler()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "d_train = d_train.reshape(-1,1)\n",
    "d_test = d_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training scaler with training data\n",
    "W_size = int(num_elem/5)\n",
    "for i in range(0,W_size*4,W_size):\n",
    "    scaler.fit(d_train[i:i+W_size,:])\n",
    "    d_train[i:i+W_size,:]=scaler.transform(d_train[i:i+W_size,:])\n",
    "\n",
    "scaler.fit(d_train[i+W_size:,:])\n",
    "d_train[i+W_size:,:] = scaler.transform(d_train[i+W_size:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping data and normaliing training data\n",
    "d_train = d_train.reshape(-1)\n",
    "d_test = scaler.transform(d_test).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building EMA for standard averaging \n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(int(num_elem*.9 )):\n",
    "    EMA = gamma*d_train[ti] + (1-gamma)*EMA\n",
    "    d_train[ti] = EMA\n",
    "m_data = np.concatenate([d_train,d_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up window of days ahead standard averaging works for\n",
    "window = 100\n",
    "N = d_train.size \n",
    "std_pred = []\n",
    "std_avg = []\n",
    "mse_errors =[]\n",
    "#training for one day ahead\n",
    "for pred in range(window,N):\n",
    "    if pred>=N:\n",
    "        date =dt.datetime.strptime(k,'%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred,'Date']\n",
    "    std_pred.append(np.mean(d_train[pred-window:pred]))\n",
    "    mse_errors.append((std_pred[-1]-d_train[pred])**2)\n",
    "    std_avg.append(date)\n",
    "    print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting prediction vs true value for standard averaging \n",
    "plt.figure(figsize = (18,9))\n",
    "plt.title(stockName,fontsize=20)\n",
    "plt.plot(range(df.shape[0]),m_data,color='b',label='True')\n",
    "plt.plot(range(window,N),std_pred,color='orange',label='Prediction')\n",
    "plt.xlabel('Num iterations')\n",
    "plt.ylabel('Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.savefig('pics/'+stockName+'__part2_prediction_to_true_Value_OnedayAhead.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up exponential averaging\n",
    "window = 100\n",
    "N = d_train.size\n",
    "avg_predictions = []\n",
    "avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "mean = 0.0\n",
    "avg_predictions.append(mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred in range(1,N):\n",
    "\n",
    "    mean = mean*decay + (1.0-decay)*d_train[pred-1]\n",
    "    avg_predictions.append(mean)\n",
    "    mse_errors.append((avg_predictions[-1]-d_train[pred])**2)\n",
    "    avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting exponential averaging\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),m_data,color='b',label='True')\n",
    "plt.plot(range(0,N),avg_predictions,color='orange', label='Prediction')\n",
    "plt.title(stockName,fontsize=20)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.savefig('pics/'+stockName+'_price_prediction_avg_PART3.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new class to use LSTMs to solve predictions for multiple days ahead\n",
    "#setting up the parameters and hyperparameters\n",
    "class d_Create(object):\n",
    "\n",
    "    def __init__(self,prices,b_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._len_prices = len(self._prices) - num_unroll\n",
    "        self._b_size = b_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._iters = self._len_prices // self._b_size\n",
    "        self._cursor = [offset * self._iters for offset in range(self._b_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        b_data = np.zeros((self._b_size), dtype=np.float32)\n",
    "        b_labels = np.zeros((self._b_size), dtype=np.float32)\n",
    "\n",
    "        for b in range(self._b_size):\n",
    "            if self._cursor[b]+1>=self._len_prices:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0, (b+1) * self._iters)\n",
    "\n",
    "            b_data[b] = self._prices[self._cursor[b]]\n",
    "            b_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._len_prices\n",
    "\n",
    "        return b_data,b_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()\n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._b_size):\n",
    "            self._cursor[b] = np.random.randint(0, min((b+1) * self._iters, self._len_prices - 1))\n",
    "\n",
    "\n",
    "\n",
    "dg = d_Create(d_train, 5, 5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for i, (dat, lbl) in enumerate(zip(u_data, u_labels)):\n",
    "    print('\\n\\nUnrolled index %d' % i)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutput:',lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " D = 1  # Dimensionality of the data\n",
    "num_unroll = 50  # Steps you look into the future.\n",
    "bat_size = 500  #  Samples in a batch\n",
    "num_nodes = [200, 200, 150]  # Hidden nodes in each layer\n",
    "n_layers = len(num_nodes)  # Number of layers\n",
    "dropout = 0.2  # dropout amount\n",
    "tf.reset_default_graph()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input data\n",
    "i_train, o_train = [], []\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "for i in range(num_unroll):\n",
    "    i_train.append(tf.placeholder(tf.float32, shape=[bat_size, D], name='train_inputs_%d' % i))\n",
    "    o_train.append(tf.placeholder(tf.float32, shape=[bat_size, 1], name='train_outputs_%d' % i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up lstm cells, and w and b\n",
    "lstm_cells = [\n",
    "        tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                                state_is_tuple=True,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                )\n",
    "for li in range(n_layers)]\n",
    "\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "lstm, input_keep_prob=1.0, output_keep_prob=1.0 - dropout, state_keep_prob=1.0 - dropout) for lstm in lstm_cells]\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('w', shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b', initializer=tf.random_uniform([1], -0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "c, h = [], []\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "    c.append(tf.Variable(tf.zeros([bat_size, num_nodes[li]]), trainable=False))\n",
    "    h.append(tf.Variable(tf.zeros([bat_size, num_nodes[li]]), trainable=False))\n",
    "    initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "    # Do  tensorflow transformations, for dynamic rnn. Also setting up the dropout \n",
    "    \n",
    "inputs = tf.concat([tf.expand_dims(t, 0) for t in i_train], axis=0)\n",
    "\n",
    "lstm_o, state = tf.nn.dynamic_rnn(\n",
    "drop_multi_cell, inputs, initial_state=tuple(initial_state),\n",
    "time_major=True, dtype=tf.float32)\n",
    "\n",
    "lstm_o = tf.reshape(lstm_o, [bat_size * num_unroll, num_nodes[-1]])\n",
    "\n",
    "outputs = tf.nn.xw_plus_b(lstm_o, w, b)\n",
    "\n",
    "split_o = tf.split(outputs, num_unroll, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sum of the mean error is the total loss \n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)] +\n",
    "                                 [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "    for i in range(num_unroll):\n",
    "        loss += tf.reduce_mean(0.5 * (split_o[i] - o_train[i]) ** 2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "gstep = tf.assign(global_step, global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None, dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None, dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "        tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "        tf_min_learning_rate)\n",
    "\n",
    "    # Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads, v = zip(*optimizer.compute_gradients(loss))\n",
    "grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "        zip(grads, v))\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Defining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(120000 / num_elem)\n",
    "\n",
    "valid_summary = 1 # Interval you make test predictions\n",
    "\n",
    "n_predict_once = 50 # Number of steps continously predicting for \n",
    "\n",
    "train_seq_length = d_train.size# Training data\n",
    "\n",
    "train_mse_ot = [] # Accumulate Train losses\n",
    "test_mse_ot = [] # Accumulate Test loss\n",
    "predictions_over_time = [] # Accumulate predictions\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Used for decaying learning rate\n",
    "loss_nondecrease_count = 0\n",
    "loss_nondecrease_threshold = 2\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Define data generator\n",
    "data_gen = d_Create(d_train,bat_size,num_unroll)\n",
    "                    \n",
    "x_axis_seq=[]\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(int(.85*num_elem),int(num_elem *.95),50).tolist()\n",
    "\n",
    "for ep in range(epochs):       \n",
    "    tmp = int(train_seq_length // bat_size)\n",
    "    # training the LSTM\n",
    "    for step in range(tmp): \n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "            feed_dict[i_train[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict[o_train[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "    # Validating the LSTM\n",
    "    if (ep+1) % valid_summary == 0:\n",
    "\n",
    "      average_loss = average_loss/(valid_summary*(train_seq_length//bat_size))\n",
    "\n",
    "      # The average loss\n",
    "      if (ep+1)%valid_summary==0:\n",
    "        print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "      train_mse_ot.append(average_loss)\n",
    "\n",
    "      average_loss = 0 # reset loss\n",
    "\n",
    "      predictions_seq = []\n",
    "\n",
    "      mse_test_loss_seq = []\n",
    "\n",
    "      # Making predictions \n",
    "    \n",
    "      for w_i in test_points_seq:\n",
    "        mse_test_loss = 0.0\n",
    "        our_predictions = []\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          x_axis=[]\n",
    "\n",
    "        for tr_i in range(w_i-num_unroll+1,w_i-1):\n",
    "          current_price = m_data[tr_i]\n",
    "          feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
    "          _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "        feed_dict = {}\n",
    "\n",
    "        current_price = m_data[w_i-1]\n",
    "\n",
    "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "\n",
    "        # Each prediction uses previous prediciton as it's current input\n",
    "        for pred_i in range(n_predict_once):\n",
    "\n",
    "          pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "          our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "          feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "          if (ep+1)-valid_summary==0:\n",
    "            \n",
    "            x_axis.append(w_i+pred_i)\n",
    "\n",
    "          mse_test_loss += 0.5*(pred-m_data[w_i+pred_i])**2\n",
    "\n",
    "        session.run(reset_sample_states)\n",
    "\n",
    "        predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "        mse_test_loss /= n_predict_once\n",
    "        mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          x_axis_seq.append(x_axis)\n",
    "\n",
    "      current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "      # Learning rate decay \n",
    "      if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "          loss_nondecrease_count += 1\n",
    "      else:\n",
    "          loss_nondecrease_count = 0\n",
    "\n",
    "      if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "            session.run(gstep)\n",
    "            loss_nondecrease_count = 0\n",
    "            print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "      test_mse_ot.append(current_test_mse)\n",
    "      print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "      predictions_over_time.append(predictions_seq)\n",
    "      print('\\tFinished Predictions')\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction = epochs-1  # best epoch with the lowest MSE \n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(df.shape[0]), m_data, color='b')\n",
    "\n",
    "            # Plotting how the predictions change over time\n",
    "start_alpha = .25\n",
    "alpha = np.arange(start_alpha, 1.1, (1.0 - start_alpha) / len(predictions_over_time[::3]))\n",
    "for p_i, p in enumerate(predictions_over_time[::3]):\n",
    "    for xval, yval in zip(x_axis_seq, p):\n",
    "        plt.plot(xval, yval, color='r', alpha=alpha[p_i])\n",
    "\n",
    "plt.title('Evolution of Test Predictions Over Time for '+stockName, fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.xlim(int(num_elem*.80),int(num_elem))\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "            # Predicting the best test prediction\n",
    "plt.plot(range(df.shape[0]), m_data, color='b')\n",
    "for xval, yval in zip(x_axis_seq, predictions_over_time[best_prediction]):\n",
    "    plt.plot(xval, yval, color='r')\n",
    "\n",
    "plt.title('Best Test Predictions Over Time for '+stockName, fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.xlim(int(num_elem*.80),int(num_elem))\n",
    "plt.savefig('pics/'+stockName+'_price_prediction_with_Apoche_part4.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
